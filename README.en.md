<div align="center">

# ğŸ”ª LLM Chunker

**Semantic text splitting powered by Large Language Models**

[![PyPI version](https://badge.fury.io/py/llm-chunker.svg)](https://badge.fury.io/py/llm-chunker)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

_Split documents by meaning, not by character count._

[Installation](#-installation) â€¢
[Quick Start](#-quick-start) â€¢
[Examples](#-examples) â€¢
[API Reference](#-api-reference) â€¢
[í•œêµ­ì–´](README.md)

</div>

---

## âœ¨ Why LLM Chunker?

Traditional chunkers split text by character count or regex patterns, often cutting sentences mid-thought. **LLM Chunker** understands contextâ€”detecting emotional shifts in novels, article boundaries in legal documents, or topic changes in podcasts.

| Traditional Chunking      | LLM Chunker                 |
| ------------------------- | --------------------------- |
| Splits by character count | Splits by semantic meaning  |
| Cuts sentences mid-word   | Preserves complete thoughts |
| One-size-fits-all         | Domain-specific prompts     |

---

## ğŸ“¦ Installation

```bash
pip install llm-chunker
```

**Requirements:**

- Python 3.8+
- OpenAI API key

---

## ğŸš€ Quick Start

```python
import os
os.environ["OPENAI_API_KEY"] = "sk-..."

from llm_chunker import GenericChunker

chunker = GenericChunker()
chunks = chunker.split_text(your_text)  # Returns list[str]
```

---

## ğŸ“– Basic Example

```python
from llm_chunker import GenericChunker

chunker = GenericChunker(
    model="gpt-4o",
    significance_threshold=7,  # Only split at significance 7+
    min_chunk_gap=200,         # Min chars between splits
    verbose=True,              # Enable detailed logging
    show_progress=True,        # Show progress + chunk results
)

chunks = chunker.split_text(your_text)  # list[str]
```

---

## ğŸ“– Custom Prompt Examples

### Method 1: Use PromptBuilder (Recommended)

```python
from llm_chunker import GenericChunker, TransitionAnalyzer, PromptBuilder

# Just specify domain and what to find - prompt is auto-generated
prompt = PromptBuilder.create(
    domain="novel",
    find="emotional shifts or scene changes",
)

analyzer = TransitionAnalyzer(
    prompt_generator=prompt,
    model="gpt-4o",
)

chunker = GenericChunker(analyzer=analyzer)
chunks = chunker.split_text(novel_text)
```

**PromptBuilder.create() Parameters:**

| Parameter            | Type  | Default              | Description                 |
| -------------------- | ----- | -------------------- | --------------------------- |
| `domain`             | `str` | `"text"`             | Domain of text to analyze   |
| `find`               | `str` | `"semantic changes"` | Type of transitions to find |
| `custom_instruction` | `str` | `None`               | Additional instructions     |

### Method 2: Use Built-in Prompts (Legal)

```python
from llm_chunker import GenericChunker, TransitionAnalyzer
from llm_chunker.prompts import get_legal_prompt

analyzer = TransitionAnalyzer(
    prompt_generator=get_legal_prompt,
    model="gpt-4o",
)

chunker = GenericChunker(analyzer=analyzer)
chunks = chunker.split_text(legal_document)
```

Additional built-in prompts to be updated

### Method 3: Write Custom Prompt Function

```python
from llm_chunker import GenericChunker, TransitionAnalyzer

def my_custom_prompt(segment: str) -> str:
    return f"""
Analyze the following text and identify points where the topic changes.

Text:
{segment}

Return JSON format:
{{
  "transition_points": [
    {{
      "start_text": "Exact quote where change begins",
      "topic_before": "Topic before this point",
      "topic_after": "Topic after this point",
      "significance": 1-10 integer,
      "explanation": "Brief explanation"
    }}
  ]
}}
""".strip()

analyzer = TransitionAnalyzer(
    prompt_generator=my_custom_prompt,
    model="gpt-4o",
)

chunker = GenericChunker(analyzer=analyzer)
chunks = chunker.split_text(your_text)
```

---

## ğŸ“š API Reference

### `GenericChunker`

| Parameter                | Type                 | Default | Description                          |
| ------------------------ | -------------------- | ------- | ------------------------------------ |
| `analyzer`               | `TransitionAnalyzer` | `None`  | Custom analyzer (prompt/model)       |
| `model`                  | `str`                | `None`  | OpenAI model name (when no analyzer) |
| `significance_threshold` | `int`                | `7`     | Min significance score (1-10)        |
| `min_chunk_gap`          | `int`                | `200`   | Min characters between splits        |
| `max_segment_size`       | `int`                | `5000`  | Segment size for LLM processing      |
| `overlap_size`           | `int`                | `400`   | Overlap between segments             |
| `verbose`                | `bool`               | `False` | Enable detailed logging              |
| `show_progress`          | `bool`               | `False` | Show progress + chunk results        |

### `TransitionAnalyzer`

| Parameter          | Type                   | Default              | Description               |
| ------------------ | ---------------------- | -------------------- | ------------------------- |
| `prompt_generator` | `Callable[[str], str]` | `get_default_prompt` | Prompt generator function |
| `model`            | `str`                  | `None`               | OpenAI model name         |

---

## ğŸ—ï¸ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Your Long Text                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. SEGMENT     Split into LLM-sized windows                 â”‚
â”‚                (max_segment_size, overlap_size applied)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ANALYZE     LLM finds transition points                  â”‚
â”‚                (Custom prompts for domain-specific analysis)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. FILTER      Remove low-significance & duplicate points   â”‚
â”‚                (significance_threshold, min_chunk_gap)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. SLICE       Split text at validated transition points    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
              [Chunk 1] [Chunk 2] [Chunk 3] ...
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE)

---

## â­ Star History

<a href="https://star-history.com/#Theeojeong/llm-chunker&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date" />
 </picture>
</a>

---

<div align="center">

**Made with â¤ï¸ for better RAG pipelines**

[â­ Star this repo](https://github.com/Theeojeong/llm-chunker) if you find it useful!

</div>
