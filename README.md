# LLM Chunker

[![PyPI version](https://badge.fury.io/py/llm-chunker.svg)](https://badge.fury.io/py/llm-chunker)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/pypi/pyversions/llm-chunker.svg)](https://pypi.org/project/llm-chunker/)

**LLM Chunker** is a semantic text splitting library that leverages Large Language Models (LLMs) to detect logical boundaries in documents. Unlike traditional chunkers that split by character count or regex, **LLM Chunker** understands the contextâ€”whether it's a shift in emotion, a new legal article, or a change in topic.

---

## ğŸŒ Language / ì–¸ì–´

- [**English**](#english)
- [**í•œêµ­ì–´ (Korean)**](#korean)

---

<div id="english"></div>

# English Description

## ğŸš€ Key Features

- **Semantic Intelligence**: Uses LLMs to find "turning points" in text where the meaning actually shifts.
- **Domain Adaptable**: Comes with specialized prompts for **Legal**, **General**, and **Narrative** texts.
- **Model Agnostic**: Built with a pluggable architecture. Use **OpenAI (default)**, **Ollama**, **Claude**, or any custom LLM function.
- **Context-Aware Overlap**: Intelligently manages overlap to preserve context across chunks.

## ğŸ“¦ Installation

```bash
pip install llm-chunker
```

## ğŸ›  Usage

### 1. Basic Usage (OpenAI)

By default, `llm-chunker` uses OpenAI's models (`gpt-4o` or `gpt-3.5-turbo`).

```python
import os
from llm_chunker import GenericChunker

# 1. Set your API Key
os.environ["OPENAI_API_KEY"] = "sk-..."

# 2. Initialize Chunker
chunker = GenericChunker()

# 3. Split Text
long_text = "..."
chunks = chunker.split_text(long_text)

for i, chunk in enumerate(chunks):
    print(f"--- Chunk {i+1} ---")
    print(chunk)
```

### 2. Advanced: Legal Document Chunking

Use specialized prompts to detect legal sections (Articles, Definitions, Purposes).

```python
from llm_chunker import GenericChunker, TransitionAnalyzer
from llm_chunker.prompts import get_legal_prompt

# Initialize Analyzer with Legal Prompt
legal_analyzer = TransitionAnalyzer(prompt_generator=get_legal_prompt)
chunker = GenericChunker(analyzer=legal_analyzer)

chunks = chunker.split_text(legal_document_text)
```

### 3. Advanced: Using Local LLM (Ollama)

Save costs and ensure privacy by running completely locally with Ollama.

```python
import ollama
from llm_chunker import GenericChunker, TransitionAnalyzer

# Define custom caller for Ollama
def ollama_caller(prompt: str) -> str:
    response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# Inject custom caller
local_analyzer = TransitionAnalyzer(
    llm_caller=ollama_caller
)

chunker = GenericChunker(analyzer=local_analyzer)
```

## ğŸ— Architecture

The library operates in three stages:

1.  **Scanning**: The text is scanned in large windows (optimized for LLM context limits).
2.  **Analysis**: The LLM analyzes the text to find "transition points" based on the provided prompt (e.g., "Find where the legal topic changes").
3.  **Slicing**: The original text is sliced precisely at these detected points, ensuring logical continuity.

### 4. Advanced: Writing Custom Prompts

You can define your own logic for splitting text (e.g., by speaker, by scene, or by timestamp) by writing a custom prompt generator.

**Important**: Your prompt **MUST** instruct the LLM to return a JSON object with a `transition_points` key. Each point must have a `start_text` field that exactly matches a snippet in the segment.

```python
from llm_chunker import GenericChunker, TransitionAnalyzer

def my_custom_prompt(segment: str) -> str:
    return f"""
    Analyze the text and find where a new speaker starts speaking.

    TEXT:
    {segment}

    Return JSON:
    {{
      "transition_points": [
        {{
          "start_text": "First 5-10 chars of the new speaker's sentence",
          "speaker": "Name of speaker",
          "significance": 10
        }}
      ]
    }}
    """

chunker = GenericChunker(analyzer=TransitionAnalyzer(prompt_generator=my_custom_prompt))
chunks = chunker.split_text(dialogue_text)
```

#### Example 2: Markdown Header Splitter

```python
def markdown_header_prompt(segment: str) -> str:
    return f"""
    Find where a new Markdown header (starts with #, ##, ###) appears.

    TEXT:
    {segment}

    Return JSON:
    {{
      "transition_points": [
        {{
          "start_text": "# Header Title",
          "significance": 10
        }}
      ]
    }}
    """
```

#### Example 3: Podcast Topic Switch

```python
def podcast_topic_prompt(segment: str) -> str:
    return f"""
    The following is a podcast transcript. Find where the host moves to a completely new topic or segment (e.g., "Moving on...", "Next topic...").

    TEXT:
    {segment}

    Return JSON:
    {{
      "transition_points": [
        {{
          "start_text": "Moving on to our next news item...",
          "topic_after": "Tech News Segment",
          "significance": 9
        }}
      ]
    }}
    """
```

---

<div id="korean"></div>

# í•œêµ­ì–´ ì„¤ëª… (Korean)

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (Semantic Chunking)**: ë‹¨ìˆœ ê¸€ì ìˆ˜ê°€ ì•„ë‹Œ, ì£¼ì œë‚˜ ë§¥ë½ì´ ë°”ë€ŒëŠ” 'ì „í™˜ì 'ì„ íŒŒì•…í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
- **ë„ë©”ì¸ íŠ¹í™” í”„ë¡¬í”„íŠ¸**: ë²•ë¥  ë¬¸ì„œ(ì¡°í•­ êµ¬ë¶„), ì†Œì„¤(ê°ì •ì„  ë³€í™”), ê¸°ìˆ  ë¬¸ì„œ ë“± ë¶„ì•¼ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- **ìœ ì—°í•œ ë°±ì—”ë“œ ì„¤ì •**: ê¸°ë³¸ì ìœ¼ë¡œ **OpenAI**ë¥¼ ì§€ì›í•˜ë©°, **Ollama(ë¡œì»¬ LLM)**ë‚˜ **Claude** ë“± ì›í•˜ëŠ” ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

```bash
pip install llm-chunker
```

## ğŸ›  ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (OpenAI)

ë³„ë„ ì„¤ì • ì—†ì´ ë°”ë¡œ OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

```python
import os
from llm_chunker import GenericChunker

# 1. API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "sk-..."

# 2. ì²­ì»¤ ì´ˆê¸°í™”
chunker = GenericChunker()

# 3. í…ìŠ¤íŠ¸ ë¶„í• 
text = "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸..."
chunks = chunker.split_text(text)

for chunk in chunks:
    print(chunk)
```

### 2. ì‹¬í™”: ë²•ë¥  ë¬¸ì„œ ìµœì í™”

ë²•ë¥  ë¬¸ì„œì˜ 'ì¡°(Article)', 'í•­(Paragraph)' êµ¬ë¶„ì„ ì •í™•íˆ ì¸ì‹í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from llm_chunker import GenericChunker, TransitionAnalyzer
from llm_chunker.prompts import get_legal_prompt

# ë²•ë¥  ì „ìš© ë¶„ì„ê¸° ìƒì„±
legal_analyzer = TransitionAnalyzer(prompt_generator=get_legal_prompt)
chunker = GenericChunker(analyzer=legal_analyzer)

chunks = chunker.split_text(legal_text)
```

### 3. ì‹¬í™”: ë¡œì»¬ LLM ì‚¬ìš© (Ollama)

API ë¹„ìš© ì—†ì´ ë¡œì»¬ì—ì„œ Llama3 ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import ollama
from llm_chunker import GenericChunker, TransitionAnalyzer

# Ollama í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜
def my_ollama_caller(prompt: str) -> str:
    resp = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
    return resp["message"]["content"]

# ë¡œì»¬ ë¶„ì„ê¸° ìƒì„±
local_analyzer = TransitionAnalyzer(
    llm_caller=my_ollama_caller
)

chunker = GenericChunker(analyzer=local_analyzer)
```

## ğŸ— ì‘ë™ ì›ë¦¬

1.  **ìŠ¤ìºë‹ (Scanning)**: ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì— ë§ëŠ” í¬ê¸°ë¡œ í›‘ìŠµë‹ˆë‹¤.
2.  **ë¶„ì„ (Analysis)**: LLMì—ê²Œ í˜„ì¬ í…ìŠ¤íŠ¸ì˜ íë¦„ì´ ë°”ë€ŒëŠ” ì§€ì (ë²•ì  ì¡°í•­ ë³€ê²½, ê°ì • ë³€í™” ë“±)ì„ ì°¾ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.
3.  **ë¶„í•  (Slicing)**: LLMì´ ì°¾ì•„ë‚¸ ì •í™•í•œ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ìë¦…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¬¸ì¥ì´ ì¤‘ê°„ì— ì˜ë¦¬ê±°ë‚˜ ë¬¸ë§¥ì´ ëŠê¸°ëŠ” í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

### 4. ì‹¬í™”: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì‘ì„±ë²• (ë‚˜ë§Œì˜ ê¸°ì¤€ ë§Œë“¤ê¸°)

í™”ì(Speaker)ê°€ ë°”ë€” ë•Œ, í˜¹ì€ ì†Œì„¤ì˜ ì¥ë©´(Scene)ì´ ì „í™˜ë  ë•Œ ë“± ë‚˜ë§Œì˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê³  ì‹¶ë‹¤ë©´ í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì‘ì„±í•˜ì„¸ìš”.

**ì¤‘ìš”**: í”„ë¡¬í”„íŠ¸ëŠ” ë°˜ë“œì‹œ LLMì—ê²Œ `transition_points` í‚¤ë¥¼ í¬í•¨í•œ JSONì„ ë°˜í™˜í•˜ë„ë¡ ì§€ì‹œí•´ì•¼ í•˜ë©°, ê° í¬ì¸íŠ¸ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” `start_text`ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from llm_chunker import GenericChunker, TransitionAnalyzer

def my_scene_prompt(segment: str) -> str:
    return f"""
    ë‹¤ìŒ ì†Œì„¤ í…ìŠ¤íŠ¸ì—ì„œ 'ì¥ë©´(Scene)'ì´ ì „í™˜ë˜ëŠ” ì§€ì ì„ ì°¾ìœ¼ì„¸ìš”.
    ì‹œê°„ì´ë‚˜ ì¥ì†Œê°€ ë°”ë€ŒëŠ” ë¬¸ì¥ì„ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤.

    í…ìŠ¤íŠ¸:
    {segment}

    ë‹¤ìŒ JSON í¬ë§·ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš” (ë§ˆí¬ë‹¤ìš´ ì—†ì´):
    {{
      "transition_points": [
        {{
          "start_text": "ì¥ë©´ì´ ë°”ë€ŒëŠ” ë¬¸ì¥ì˜ ì²« 5~10ê¸€ì (ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨)",
          "reason": "ì¥ì†Œê°€ ê±°ì‹¤ì—ì„œ ë¶€ì—Œìœ¼ë¡œ ë°”ë€œ",
          "significance": 10
        }}
      ]
    }}
    """

# ë‚´ë§˜ëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ ì ìš©
my_analyzer = TransitionAnalyzer(prompt_generator=my_scene_prompt)
chunker = GenericChunker(analyzer=my_analyzer)

chunks = chunker.split_text(novel_text)
```

#### ì˜ˆì‹œ 2: ë§ˆí¬ë‹¤ìš´ í—¤ë”(Header) ê¸°ì¤€ ë¶„í• 

```python
def markdown_header_prompt(segment: str) -> str:
    return f"""
    í…ìŠ¤íŠ¸ì—ì„œ ë§ˆí¬ë‹¤ìš´ í—¤ë”(#, ##, ### ë“±)ê°€ ì‹œì‘ë˜ëŠ” ì§€ì ì„ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”.

    í…ìŠ¤íŠ¸:
    {segment}

    JSON ë°˜í™˜:
    {{
      "transition_points": [
        {{
          "start_text": "## 2. ì„¤ì¹˜ ë°©ë²•",
          "significance": 10
        }}
      ]
    }}
    """
```

#### ì˜ˆì‹œ 3: íŒŸìºìŠ¤íŠ¸/ìœ íŠœë¸Œ ì£¼ì œ ì „í™˜

```python
def podcast_topic_prompt(segment: str) -> str:
    return f"""
    ë‹¤ìŒ íŒŸìºìŠ¤íŠ¸ ëŒ€ë³¸ì—ì„œ ëŒ€í™”ì˜ ì£¼ì œê°€ ì™„ì „íˆ ë°”ë€Œê±°ë‚˜ ìƒˆë¡œìš´ ì½”ë„ˆë¡œ ë„˜ì–´ê°€ëŠ” ì§€ì ì„ ì°¾ìœ¼ì„¸ìš”.
    (ì˜ˆ: "ì, ë‹¤ìŒ ì£¼ì œë¡œ ë„˜ì–´ê°€ ë³¼ê¹Œìš”?", "ì´ì œ ê´‘ê³  ë“£ê³  ì˜¤ê² ìŠµë‹ˆë‹¤" ë“±)

    í…ìŠ¤íŠ¸:
    {segment}

    JSON ë°˜í™˜:
    {{
      "transition_points": [
        {{
          "start_text": "ì, ê·¸ëŸ¼ ì´ì œ ê²½ì œ ë‰´ìŠ¤ë¥¼ ì‚´í´ë³¼ê¹Œìš”?",
          "topic_after": "ê²½ì œ ë‰´ìŠ¤ ì½”ë„ˆ",
          "significance": 9
        }}
      ]
    }}
    """
```

---

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

## Star History

<a href="https://star-history.com/#Theeojeong/llm-chunker&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date" />
 </picture>
</a>
