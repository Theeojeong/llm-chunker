# LLM Chunker

[![PyPI version](https://badge.fury.io/py/llm-chunker.svg)](https://badge.fury.io/py/llm-chunker)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/pypi/pyversions/llm-chunker.svg)](https://pypi.org/project/llm-chunker/)

**LLM Chunker** is a semantic text splitting library that leverages Large Language Models (LLMs) to detect logical boundaries in documents. Unlike traditional chunkers that split by character count or regex, **LLM Chunker** understands the contextâ€”whether it's a shift in emotion, a new legal article, or a change in topic.

---

## ğŸŒ Language / ì–¸ì–´

- [**English**](#english)
- [**í•œêµ­ì–´ (Korean)**](#korean)

---

<div id="english"></div>

# English Description

## ğŸš€ Key Features

- **Semantic Intelligence**: Uses LLMs to find "turning points" in text where the meaning actually shifts.
- **Domain Adaptable**: Comes with specialized prompts for **Legal**, **General**, and **Narrative** texts.
- **Model Agnostic**: Built with a pluggable architecture. Use **OpenAI (default)**, **Ollama**, **Claude**, or any custom LLM function.
- **Context-Aware Overlap**: Intelligently manages overlap to preserve context across chunks.

## ğŸ“¦ Installation

```bash
pip install llm-chunker
```

## ğŸ›  Usage

### 1. Basic Usage (OpenAI)

By default, `llm-chunker` uses OpenAI's models (`gpt-4o` or `gpt-3.5-turbo`).

```python
import os
from llm_chunker import GenericChunker

# 1. Set your API Key
os.environ["OPENAI_API_KEY"] = "sk-..."

# 2. Initialize Chunker
chunker = GenericChunker()

# 3. Split Text
long_text = "..."
chunks = chunker.split_text(long_text)

for i, chunk in enumerate(chunks):
    print(f"--- Chunk {i+1} ---")
    print(chunk)
```

### 2. Advanced: Legal Document Chunking

Use specialized prompts to detect legal sections (Articles, Definitions, Purposes).

```python
from llm_chunker import GenericChunker, TransitionAnalyzer
from llm_chunker.prompts import get_legal_prompt

# Initialize Analyzer with Legal Prompt
legal_analyzer = TransitionAnalyzer(prompt_generator=get_legal_prompt)
chunker = GenericChunker(analyzer=legal_analyzer)

chunks = chunker.split_text(legal_document_text)
```

### 3. Advanced: Using Local LLM (Ollama)

Save costs and ensure privacy by running completely locally with Ollama.

```python
import ollama
from llm_chunker import GenericChunker, TransitionAnalyzer

# Define custom caller for Ollama
def ollama_caller(prompt: str) -> str:
    response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# Inject custom caller
local_analyzer = TransitionAnalyzer(
    llm_caller=ollama_caller
)

chunker = GenericChunker(analyzer=local_analyzer)
```

## ğŸ— Architecture

The library operates in three stages:

1.  **Scanning**: The text is scanned in large windows (optimized for LLM context limits).
2.  **Analysis**: The LLM analyzes the text to find "transition points" based on the provided prompt (e.g., "Find where the legal topic changes").
3.  **Slicing**: The original text is sliced precisely at these detected points, ensuring logical continuity.

---

<div id="korean"></div>

# í•œêµ­ì–´ ì„¤ëª… (Korean)

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (Semantic Chunking)**: ë‹¨ìˆœ ê¸€ì ìˆ˜ê°€ ì•„ë‹Œ, ì£¼ì œë‚˜ ë§¥ë½ì´ ë°”ë€ŒëŠ” 'ì „í™˜ì 'ì„ íŒŒì•…í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
- **ë„ë©”ì¸ íŠ¹í™” í”„ë¡¬í”„íŠ¸**: ë²•ë¥  ë¬¸ì„œ(ì¡°í•­ êµ¬ë¶„), ì†Œì„¤(ê°ì •ì„  ë³€í™”), ê¸°ìˆ  ë¬¸ì„œ ë“± ë¶„ì•¼ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- **ìœ ì—°í•œ ë°±ì—”ë“œ ì„¤ì •**: ê¸°ë³¸ì ìœ¼ë¡œ **OpenAI**ë¥¼ ì§€ì›í•˜ë©°, **Ollama(ë¡œì»¬ LLM)**ë‚˜ **Claude** ë“± ì›í•˜ëŠ” ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

```bash
pip install llm-chunker
```

## ğŸ›  ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (OpenAI)

ë³„ë„ ì„¤ì • ì—†ì´ ë°”ë¡œ OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

```python
import os
from llm_chunker import GenericChunker

# 1. API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "sk-..."

# 2. ì²­ì»¤ ì´ˆê¸°í™”
chunker = GenericChunker()

# 3. í…ìŠ¤íŠ¸ ë¶„í• 
text = "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸..."
chunks = chunker.split_text(text)

for chunk in chunks:
    print(chunk)
```

### 2. ì‹¬í™”: ë²•ë¥  ë¬¸ì„œ ìµœì í™”

ë²•ë¥  ë¬¸ì„œì˜ 'ì¡°(Article)', 'í•­(Paragraph)' êµ¬ë¶„ì„ ì •í™•íˆ ì¸ì‹í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from llm_chunker import GenericChunker, TransitionAnalyzer
from llm_chunker.prompts import get_legal_prompt

# ë²•ë¥  ì „ìš© ë¶„ì„ê¸° ìƒì„±
legal_analyzer = TransitionAnalyzer(prompt_generator=get_legal_prompt)
chunker = GenericChunker(analyzer=legal_analyzer)

chunks = chunker.split_text(legal_text)
```

### 3. ì‹¬í™”: ë¡œì»¬ LLM ì‚¬ìš© (Ollama)

API ë¹„ìš© ì—†ì´ ë¡œì»¬ì—ì„œ Llama3 ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import ollama
from llm_chunker import GenericChunker, TransitionAnalyzer

# Ollama í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜
def my_ollama_caller(prompt: str) -> str:
    resp = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
    return resp["message"]["content"]

# ë¡œì»¬ ë¶„ì„ê¸° ìƒì„±
local_analyzer = TransitionAnalyzer(
    llm_caller=my_ollama_caller
)

chunker = GenericChunker(analyzer=local_analyzer)
```

## ğŸ— ì‘ë™ ì›ë¦¬

1.  **ìŠ¤ìºë‹ (Scanning)**: ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì— ë§ëŠ” í¬ê¸°ë¡œ í›‘ìŠµë‹ˆë‹¤.
2.  **ë¶„ì„ (Analysis)**: LLMì—ê²Œ í˜„ì¬ í…ìŠ¤íŠ¸ì˜ íë¦„ì´ ë°”ë€ŒëŠ” ì§€ì (ë²•ì  ì¡°í•­ ë³€ê²½, ê°ì • ë³€í™” ë“±)ì„ ì°¾ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.
3.  **ë¶„í•  (Slicing)**: LLMì´ ì°¾ì•„ë‚¸ ì •í™•í•œ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ìë¦…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¬¸ì¥ì´ ì¤‘ê°„ì— ì˜ë¦¬ê±°ë‚˜ ë¬¸ë§¥ì´ ëŠê¸°ëŠ” í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

---

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

## Star History

<a href="https://star-history.com/#Theeojeong/llm-chunker&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Theeojeong/llm-chunker&type=Date" />
 </picture>
</a>
